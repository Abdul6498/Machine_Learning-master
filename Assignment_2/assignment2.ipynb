{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes and Text Classification\n",
    "\n",
    "Only use the already imported library `numpy`. Make sure that the `liar.txt` dataset is in the same directory as the notebook.\n",
    "\n",
    "List your team members (name and immatriculation number) in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Your names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "real: ['thanks', 'to', 'our', 'reforms', 'the', 'average', 'family', 'will', 'have', 'an', 'extra', '322', 'to', 'spend']\n",
      "real: ['winning', 'enough', 'pledged', 'delegates', 'is', 'not', 'impossible']\n"
     ]
    }
   ],
   "source": [
    "# Load required packages and dataset. Do not modify.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_liar_dataset():\n",
    "    import string\n",
    "    \n",
    "    with open('liar.txt', mode='r', encoding='utf-8') as f:\n",
    "        rows = [l.strip().split('\\t')[:2] for l in f]\n",
    "    \n",
    "    y, X = zip(*rows)\n",
    "    X =[x.translate(str.maketrans('', '', string.punctuation)).lower().split() for x in X]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "X, y = load_liar_dataset()\n",
    "\n",
    "print('Sample:')\n",
    "print(f'{y[0]}: {X[0]}')\n",
    "print(f'{y[2]}: {X[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fake News Classification with Naive Bayes\n",
    "\n",
    "Implement a Naive Bayes classifier with Laplace smoothing to detect whether a text message is fake or real (not fake).\n",
    "\n",
    "A text message is represented by a list of string tokens as shown above.\n",
    "The classification target is binary and the two possible labels are the strings `'fake'` and `'real'`.\n",
    "\n",
    "Fill out the methods in `NaiveBayesFakeNewsClassifier` to train (`fit`) and predict (`predict`). Feel free to introduce new fields and methods based on your needs, but the methods `fit` and `predict` are required and their interface should not be changed.\n",
    "\n",
    "Hint: Try to map the text messages to word frequency vectors by counting how often each word occurs in a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here.\n",
    "class NaiveBayesFakeNewsClassifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X is a list of `n` text messages. Each text message is a list of strings with at least length one.\n",
    "        y is a list of `n` labels either the string 'fake' or the string 'real'.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is a list of `n` text messages. Each text message is a list of strings with at least length one.\n",
    "        The method returns a list of `n` strings, i.e. classification labels ('fake' or 'real').\n",
    "        \"\"\"\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected array-like (array or non-string sequence), got None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-95d310f0e7a1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0myour_classifier\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNaiveBayesFakeClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0mreal_cmatrix\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreal_classifier\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m \u001B[0myour_cmatrix\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0myour_classifier\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0mplot_confusion_matrix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreal_cmatrix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'RealClassifier'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-95d310f0e7a1>\u001B[0m in \u001B[0;36mtrain_evaluate\u001B[0;34m(classifier, X, y)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;31m# Evaluate classifier on test data.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0myhat_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m     \u001B[0mcmatrix\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfusion_matrix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myhat_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'real'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'fake'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mcmatrix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/untitled/venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/untitled/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36mconfusion_matrix\u001B[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m     \"\"\"\n\u001B[0;32m--> 299\u001B[0;31m     \u001B[0my_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    300\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0my_type\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"binary\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"multiclass\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    301\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%s is not supported\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0my_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/untitled/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     83\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m     \u001B[0mtype_true\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m     \u001B[0mtype_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[0my_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mtype_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype_pred\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/untitled/venv/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001B[0m in \u001B[0;36mtype_of_target\u001B[0;34m(y)\u001B[0m\n\u001B[1;32m    253\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mvalid\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    254\u001B[0m         raise ValueError('Expected array-like (array or non-string sequence), '\n\u001B[0;32m--> 255\u001B[0;31m                          'got %r' % y)\n\u001B[0m\u001B[1;32m    256\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    257\u001B[0m     \u001B[0msparse_pandas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'SparseSeries'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'SparseArray'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Expected array-like (array or non-string sequence), got None"
     ]
    }
   ],
   "source": [
    "# The following code will evaluate your classifier.\n",
    "class RealClassifier(object):\n",
    "    \"\"\"\n",
    "    This classifier is a primitive baseline, which just predicts the most common class each time.\n",
    "    Naive Bayes should definitely beat this.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y): pass\n",
    "    def predict(self, X): return len(X)*['real']\n",
    "\n",
    "    \n",
    "def train_evaluate(classifier, X, y):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Apply train-test split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "    # Inititialize and train classifier.\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Evaluate classifier on test data.\n",
    "    yhat_test = classifier.predict(X_test)\n",
    "    cmatrix = confusion_matrix(y_test, yhat_test, labels=['real', 'fake'])\n",
    "    \n",
    "    return cmatrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cmatrix, classifier_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.matshow(cmatrix, cmap='Greens')\n",
    "    for x in (0, 1):\n",
    "        for y in (0, 1):\n",
    "            ax.text(x, y, cmatrix[y, x])\n",
    "    ax.set_xlabel('predicted label')\n",
    "    ax.set_ylabel('true label')\n",
    "    ax.set_xticklabels(['', 'real', 'fake'])\n",
    "    ax.set_yticklabels(['', 'real', 'fake'])\n",
    "    ax.set_title(classifier_name)\n",
    "\n",
    "    \n",
    "    \n",
    "real_classifier = RealClassifier()\n",
    "your_classifier = NaiveBayesFakeNewsClassifier()\n",
    "real_cmatrix = train_evaluate(real_classifier, X, y)\n",
    "your_cmatrix = train_evaluate(your_classifier, X, y)\n",
    "\n",
    "plot_confusion_matrix(real_cmatrix, 'RealClassifier')\n",
    "plot_confusion_matrix(your_cmatrix, 'NaiveBayesFakeNewsClassifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}