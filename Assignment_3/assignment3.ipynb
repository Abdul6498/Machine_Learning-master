{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grand-department",
   "metadata": {},
   "source": [
    "# Assignment 3: Linear Regression (100 points)\n",
    "\n",
    "Only use the already imported library `numpy` and the Python standard library. Make sure that the datasets `dataLinReg2D.txt`, `dataQuadReg2D.txt` and `dataQuadReg2D_noisy.txt` are in the same directory as the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-portugal",
   "metadata": {},
   "source": [
    "==> *Write*\n",
    "* *names* \n",
    "* *matr. nr.* \n",
    "* *study program*\n",
    "* *B.Sc./M.Sc.*\n",
    "\n",
    "*of all assignment group participants here. (double klick here to edit)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-copper",
   "metadata": {},
   "source": [
    "## Task 1: Linear Regression - theory (10 points)\n",
    "1) *(4 points)* Linear regression can have nonlinear input features, why is it still called linear regression? In what sense is it linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-columbia",
   "metadata": {},
   "source": [
    "==> *Write your response here.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-generic",
   "metadata": {},
   "source": [
    "2) *(3 points)* For calculating optimal parameters $\\hat{\\beta}$ the inverse of $X^{\\top}X$ has to be calculated. When would this matrix be singular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-reader",
   "metadata": {},
   "source": [
    "==> *Write your response here.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-blake",
   "metadata": {},
   "source": [
    "3) *(3 points)* Why does the optimization problem for $L_1$-regularization (Lasso) not have a closed form solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-connectivity",
   "metadata": {},
   "source": [
    "==> *Write your response here.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-stations",
   "metadata": {},
   "source": [
    "## Task 2: Ridge Regression for Polynomial 2D Functions (70 points)\n",
    "Each line in the data sets consists of a data entry `(x,y)` with a 2D point `x` and a 1D function output `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wired-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages and datasets. Do not modify.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = np.loadtxt(path)\n",
    "    X, y = data[:, :2], data[:, 2]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "X_lin, y_lin = load_dataset(\"dataLinReg2D.txt\")\n",
    "X_quad, y_quad = load_dataset(\"dataQuadReg2D.txt\")\n",
    "X_noise, y_noise = load_dataset(\"dataQuadReg2D_noisy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-freedom",
   "metadata": {},
   "source": [
    "1) *(5 points)* Draw random samples from the dataset for training, use 70% of the data. Do not modify the interface of the function. Do this for `X_quad` and `X_noise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modern-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(X, y, test_split = 0.7):\n",
    "    \"\"\"\n",
    "    Returns X_train, y_train\n",
    "        where X_train are random samples of X and y_train are the corresponding true values.\n",
    "        test_split represents the persentage of how many training samples are drawn from X.\n",
    "    \"\"\"\n",
    "    # Implement your solution here.\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "X_quad_train, y_quad_train = train_split(X_quad, y_quad)\n",
    "X_noise_train, y_noise_train = train_split(X_noise, y_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-ladder",
   "metadata": {},
   "source": [
    "2) *(10 points)* Extend the `make_features` method to also compute quadratic features (`ftype = 'quad'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "selective-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X, ftype='lin'):\n",
    "    \"\"\"\n",
    "    generates features from input data, returns Phi.\n",
    "    ftype is used to distinguish types of features\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    if ftype == 'lin': \n",
    "        # Linear feature transformation (including intercept)\n",
    "        Phi = np.empty((n, d+1))\n",
    "        Phi[:, 0] = 1\n",
    "        Phi[:, 1:] = X\n",
    "        \n",
    "    elif ftype == 'quad':\n",
    "        # Quadratic feature transformation\n",
    "        \n",
    "        # Implement your solution here.\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f'Feature type {ftype} not implemented yet')\n",
    "    \n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-texture",
   "metadata": {},
   "source": [
    "3) *(10 points)* Implement Ridge Regression to fit a polynomial function to the data sets with the regularization parameter `lambda_` and feature type `ftype`.\n",
    "\n",
    "Fill out the methods in `RidgeRegression` to train (`fit`) and predict (`predict`). Feel free to introduce new fields and methods based on your needs, but the methods `fit` and `predict` are required and their interface should not be changed. You need to store the vector of regression coefficients in the field `self.beta`. Before calculating the inverse check if the determinant is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "public-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    def __init__(self, lambda_, ftype = 'lin'):\n",
    "        self.lambda_ = lambda_\n",
    "        self.ftype = ftype\n",
    "        self.beta = None  # Learned regression coefficients.\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X is an array of shape (n, d), \n",
    "            where n is the number of samples and d is the number of features.\n",
    "        y is an array of shape (n,)\n",
    "        \"\"\"\n",
    "        \n",
    "        Phi = make_features(X, self.ftype)\n",
    "        \n",
    "        # Implement your solution here.\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is an array with shape (n, d).\n",
    "        The method returns an array of shape (n,).\n",
    "        \"\"\"\n",
    "        Phi = make_features(X, self.ftype)\n",
    "        \n",
    "        # Implement your solution here.\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-revolution",
   "metadata": {},
   "source": [
    "4) *(5 points)* Implement the function `MSE` to compute the mean squared error. `y_pred` and `y_true` are the vectors of predicted and true function outputs respectively with shape `(n,)`, where `n` is the number of samples. The function returns a single float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    return the mean squared error of y_pred and y_true\n",
    "    \"\"\"\n",
    "    # Implement your solution here.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-plenty",
   "metadata": {},
   "source": [
    "5) *(30 points)* Evaluate your Ridge Regression model with linear features on the linear `(X_lin, y_lin)` data set. Report the MSE on the full data set when trained on the full dataset. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis. print the minimal `lambda_`.\n",
    "\n",
    "How does it perform with quadratic features on this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worst-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_evaluate(regression_model, X, y):\n",
    "    \"\"\"\n",
    "    Use X and y to fit the regression_model and make prediction over the same dataset.\n",
    "    Print the error\n",
    "    \"\"\"\n",
    "    regression_model.fit(X, y)\n",
    "    yhat = regression_model.predict(X)\n",
    "    print(f'MSE: {MSE(yhat, y)}, feature type: {regression_model.ftype}')\n",
    "    \n",
    "def plot_data_and_model(regression_model, X, y):\n",
    "    \"\"\"\n",
    "    Generates a 3D plot of the regression result including the true underlying data.\n",
    "    The data points are indicated by circles, the prediction is shown as a surface\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.suptitle(f'{regression_model.ftype} Features')\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], y, marker = 'o')\n",
    "    \n",
    "    x_min = X.min(0)\n",
    "    x_max = X.max(0)\n",
    "\n",
    "    x0_grid, x1_grid = np.mgrid[x_min[0]:x_max[0]:.3, x_min[1]:x_max[1]:.3]\n",
    "\n",
    "    x_dim_0, x_dim_1 = np.shape(x0_grid)\n",
    "    x_size = np.size(x0_grid)\n",
    "\n",
    "    x0_hat = x0_grid.flatten()\n",
    "    x1_hat = x1_grid.flatten()\n",
    "    x0_hat = x0_hat.reshape((np.size(x0_hat), 1))\n",
    "    x1_hat = x1_hat.reshape((np.size(x1_hat), 1))\n",
    "    x_hat = np.append(x0_hat, x1_hat, 1)\n",
    "    x_hat_fv = make_features(x_hat, regression_model.ftype)\n",
    "    y_hat = x_hat_fv.dot(regression_model.beta)\n",
    "    y_grid = y_hat.reshape((x_dim_0, x_dim_1))\n",
    "    ax.plot_wireframe(x0_grid, x1_grid, y_grid)\n",
    "    ax.auto_scale_xyz([x_min[0], x_max[0]], [x_min[1], x_max[1]], [y.min(), y.max()])\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_zlabel('$y$')\n",
    "    \n",
    "# Implement your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-stylus",
   "metadata": {},
   "source": [
    "6) *(5 points)* Evaluate the quadratic dataset `(X_quad, y_quad)` for different values of `lambda_`. Report the MSE on the full data set when trained on the partial dataset `(X_quad_train, y_quad_train)`. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis. print the minimal `lambda_`.\n",
    "\n",
    "Plot the surface and data points of the best `lambda_` value using the function `plot_data_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "present-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-avatar",
   "metadata": {},
   "source": [
    "7) *(5 points)* Evaluate the noisy dataset `(X_noise, y_noise)` for different values of `lambda_`. Report the MSE on the full data set when trained on the partial dataset `(X_noise_train, y_noise_train)`. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis.\n",
    "\n",
    "Plot the surface and data points of the best `lambda_` value using the function `plot_data_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "functioning-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-balloon",
   "metadata": {},
   "source": [
    "## Task 3 Evaluation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-pattern",
   "metadata": {},
   "source": [
    "1) *(5 points)* What was the best choice for regularization term `lambda_` in the models above? Explain the observation from the previous task? If `lambda_` is set to zero $\\hat{\\beta}$ is not regularized, when would $\\lambda = 0$ be a good choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-bhutan",
   "metadata": {},
   "source": [
    "==> *Briefly explain the observation from the previous task.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-school",
   "metadata": {},
   "source": [
    "**For all students other than B.Sc. Data Science:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-wright",
   "metadata": {},
   "source": [
    "2) *(15 points)* Implement the function `cross_validation` for `k_fold = 10` to evaluate the prediction error of your model. Report the mean squared error from cross-validation. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "refined-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(regression_model, X, y, k_fold = 10):\n",
    "    \"\"\"\n",
    "    partition data X in k_fold equal sized subsets D = {D_1, ..., D_{k_fold}}, \n",
    "        fit the model on k_fold-1 subsets (D\\D_i), \n",
    "        compute MSE on the evaluatin set (D_i),\n",
    "        return the mean MSE over all subsets D\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-return",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
