{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grand-department",
   "metadata": {},
   "source": [
    "# Assignment 3: Linear Regression (100 points)\n",
    "\n",
    "Only use the already imported library `numpy` and the Python standard library. Make sure that the datasets `dataLinReg2D.txt`, `dataQuadReg2D.txt` and `dataQuadReg2D_noisy.txt` are in the same directory as the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-portugal",
   "metadata": {},
   "source": [
    "==> *Write*\n",
    "* *names* \n",
    "* *matr. nr.* \n",
    "* *study program*\n",
    "* *B.Sc./M.Sc.*\n",
    "\n",
    "*of all assignment group participants here. (double klick here to edit)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-copper",
   "metadata": {},
   "source": [
    "## Task 1: Linear Regression - theory (10 points)\n",
    "1) *(4 points)* Linear regression can have nonlinear input features, why is it still called linear regression? In what sense is it linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-columbia",
   "metadata": {},
   "source": [
    "==> *Write your response here.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-generic",
   "metadata": {},
   "source": [
    "2) *(3 points)* For calculating optimal parameters $\\hat{\\beta}$ the inverse of $X^{\\top}X$ has to be calculated. When would this matrix be singular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-reader",
   "metadata": {},
   "source": [
    "==> *Write your response here.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-blake",
   "metadata": {},
   "source": [
    "3) *(3 points)* Why does the optimization problem for $L_1$-regularization (Lasso) not have a closed form solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-connectivity",
   "metadata": {},
   "source": [
    "==> *Write your response here.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-stations",
   "metadata": {},
   "source": [
    "## Task 2: Ridge Regression for Polynomial 2D Functions (70 points)\n",
    "Each line in the data sets consists of a data entry `(x,y)` with a 2D point `x` and a 1D function output `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wired-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages and datasets. Do not modify.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = np.loadtxt(path)\n",
    "    X, y = data[:, :2], data[:, 2]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "X_lin, y_lin = load_dataset(\"dataLinReg2D.txt\")\n",
    "X_quad, y_quad = load_dataset(\"dataQuadReg2D.txt\")\n",
    "X_noise, y_noise = load_dataset(\"dataQuadReg2D_noisy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-freedom",
   "metadata": {},
   "source": [
    "1) *(5 points)* Draw random samples from the dataset for training, use 70% of the data. Do not modify the interface of the function. Do this for `X_quad` and `X_noise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "modern-field",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70 [ 1.45131  2.10077  5.2415  14.2015   6.28687  2.63999  1.65735  1.84973\n  1.5936   5.93914 17.3139   2.79152  1.28165  4.97582  2.06822  4.29598\n  2.12421  4.51654  2.06546  1.49823  5.86448  1.5392   3.82217  5.08817\n  9.51408  1.30278  5.44264  2.15566  1.59536  6.75806 13.5923  23.663\n  2.43735  6.75188  1.70615  4.46588  1.80535  2.59595  5.02799  2.58967\n  4.06867  3.45738 15.1917   6.39383  4.25406  3.31947  5.01941  4.23633\n  2.2513   1.54746  3.86675  1.88672  1.5219   7.55069  4.67213  1.75698\n  4.43193  3.25097  1.9744   2.60432  1.85641  1.9954   4.95341  4.0708\n  2.57071  1.58166  1.19342  3.78088  1.53254  4.48272  3.98014  2.71951\n  1.15942  1.17415  6.41083  3.80554  1.53079  1.05577  7.62581  8.16751\n  1.80186  1.91427  1.46729  5.22764  2.11754  1.77624  5.10234  1.32959\n  3.03413  2.16991  2.05367  1.30963  4.14644  3.90395  1.84863  1.59201\n  1.1192   4.46245  2.40571  5.75911]\n35 35\n"
     ]
    }
   ],
   "source": [
    "def train_split(X, y, test_split = 0.7):\n",
    "    \"\"\"\n",
    "    Returns X_train, y_train\n",
    "        where X_train are random samples of X and y_train are the corresponding true values.\n",
    "        test_split represents the persentage of how many training samples are drawn from X.\n",
    "    \"\"\"\n",
    "    # Implement your solution here.\n",
    "    np.random.seed(2020)  # Ensure that the random split always returns the same result.\n",
    "    \n",
    "    threshold = int(test_split*X.shape[0])\n",
    "    rnd_idx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    X_train = X[rnd_idx[:threshold]]\n",
    "#    X_test = X[rnd_idx[threshold:]]\n",
    "    y_train = y[rnd_idx[:threshold]]\n",
    "#    y_test = y[rnd_idx[threshold:]]\n",
    "    return X_train, y_train\n",
    "\n",
    "X_quad_train, y_quad_train = train_split(X_quad, y_quad)\n",
    "X_noise_train, y_noise_train = train_split(X_noise, y_noise)\n",
    "print(len(X_quad_train), y_quad)\n",
    "print(len(X_noise_train), len(y_noise_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-ladder",
   "metadata": {},
   "source": [
    "2) *(10 points)* Extend the `make_features` method to also compute quadratic features (`ftype = 'quad'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "selective-macintosh",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6 [9.49909485e-01 1.03910167e-01 1.83843769e+00 7.39127076e-03\n 4.14220960e-01 7.84677072e-01 3.27985290e-03 1.73317225e+00\n 1.50528361e+00 9.46995993e-02 4.07882455e+00 1.20082348e-01\n 3.89465861e-01 2.26006122e+00 1.01670922e+00 6.84900304e+00\n 5.13711291e+00 1.71332392e+00 1.19010645e+00 1.44110421e+00\n 6.08997627e-01 1.01082916e-02 1.48172191e+00 5.43043791e+00\n 1.63054440e-01 1.82279701e+00 2.31843257e+00 3.74984770e-01\n 8.73422954e-03 3.63349756e-01 2.22695929e+00 9.48037164e-01\n 2.95537926e-03 7.46760407e-03 7.70127350e-03 2.86151056e-02\n 3.61193826e-02 5.03261922e+00 5.24432900e+00 1.42284341e+00\n 1.70566212e-04 2.27352115e+00 2.45059651e-01 6.63376687e+00\n 3.03378437e-01 1.10795780e-01 3.61060380e-01 9.35535807e-01\n 7.45753145e-01 4.01918294e+00 2.47873536e+00 1.54370685e+00\n 2.03096701e-02 3.63237857e-02 5.14918186e-03 7.18825883e-01\n 6.82426803e+00 2.25792697e-02 1.02959580e+00 1.51457326e-02\n 1.56635237e+00 3.28298059e-01 5.54677884e-01 5.88804537e-01\n 7.50814801e-02 1.50118277e-03 1.89480731e+00 6.02407936e-02\n 1.02123151e+00 7.12179957e-02 4.88086671e-01 2.37600678e-01\n 1.23223320e-02 4.73449958e-01 2.31831076e+00 5.14423176e-01\n 1.44774162e-01 1.01578178e+00 1.41531707e-01 3.16348910e+00\n 4.48435140e-01 2.29428546e-01 7.60099386e-02 3.44232362e-02\n 5.75688042e+00 2.28938326e-01 2.30274555e-04 3.51300049e+00\n 1.20763200e-03 1.94056108e-01 1.99250163e+00 4.17613747e+00\n 3.85521444e+00 4.05947380e-01 3.60570728e-02 1.87498249e+00\n 2.78202384e+00 5.59201849e-03 8.57078321e-02 5.33457551e-02]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,100) into shape (100,3)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-e75da0c2ee67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mPhi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_lin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-e75da0c2ee67>\u001b[0m in \u001b[0;36mmake_features\u001b[1;34m(X, ftype)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mPhi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mPhi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mPhi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_1_sq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_1_x_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_2_sq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Implement your solution here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3,100) into shape (100,3)"
     ]
    }
   ],
   "source": [
    "def make_features(X, ftype='quad'):\n",
    "    \"\"\"\n",
    "    generates features from input data, returns Phi.\n",
    "    ftype is used to distinguish types of features\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    if ftype == 'lin': \n",
    "        # Linear feature transformation (including intercept)\n",
    "        #(Rows, Columns)\n",
    "        #(100 rows, 3 columns)\n",
    "        Phi = np.empty((n, d+1))\n",
    "        Phi[:, 0] = 1\n",
    "        Phi[:, 1:] = X\n",
    "        \n",
    "    elif ftype == 'quad':\n",
    "        # Quadratic feature transformation\n",
    "        dim = int(1 + (d + (d*(d + 1))/2))\n",
    "        temp_x_1 = X[:, 0]\n",
    "        temp_x_2 = X[:, 1]\n",
    "        x_1_sq = temp_x_1*temp_x_1\n",
    "        x_1_x_2 = temp_x_1*temp_x_2\n",
    "        x_2_sq = temp_x_2 * temp_x_2\n",
    "        feat = np.array(x_1_sq, x_1_x_2, x_2_sq)\n",
    "        print(dim, x_1_sq)\n",
    "        Phi = np.empty((n, dim))\n",
    "        Phi[:, 0] = 1\n",
    "        Phi[:, 1:d+1] = X\n",
    "        Phi[:, d+1:] = 0\n",
    "        \n",
    "        # Implement your solution here.\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f'Feature type {ftype} not implemented yet')\n",
    "    \n",
    "    return Phi\n",
    "feat = make_features(X_lin)\n",
    "print(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-texture",
   "metadata": {},
   "source": [
    "3) *(10 points)* Implement Ridge Regression to fit a polynomial function to the data sets with the regularization parameter `lambda_` and feature type `ftype`.\n",
    "\n",
    "Fill out the methods in `RidgeRegression` to train (`fit`) and predict (`predict`). Feel free to introduce new fields and methods based on your needs, but the methods `fit` and `predict` are required and their interface should not be changed. You need to store the vector of regression coefficients in the field `self.beta`. Before calculating the inverse check if the determinant is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "public-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    def __init__(self, lambda_, ftype = 'lin'):\n",
    "        self.lambda_ = lambda_\n",
    "        self.ftype = ftype\n",
    "        self.beta = None  # Learned regression coefficients.\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X is an array of shape (n, d), \n",
    "            where n is the number of samples and d is the number of features.\n",
    "        y is an array of shape (n,)\n",
    "        \"\"\"\n",
    "        \n",
    "        Phi = make_features(X, self.ftype)\n",
    "        \n",
    "        # Implement your solution here.\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is an array with shape (n, d).\n",
    "        The method returns an array of shape (n,).\n",
    "        \"\"\"\n",
    "        Phi = make_features(X, self.ftype)\n",
    "        \n",
    "        # Implement your solution here.\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-revolution",
   "metadata": {},
   "source": [
    "4) *(5 points)* Implement the function `MSE` to compute the mean squared error. `y_pred` and `y_true` are the vectors of predicted and true function outputs respectively with shape `(n,)`, where `n` is the number of samples. The function returns a single float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    return the mean squared error of y_pred and y_true\n",
    "    \"\"\"\n",
    "    # Implement your solution here.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-plenty",
   "metadata": {},
   "source": [
    "5) *(30 points)* Evaluate your Ridge Regression model with linear features on the linear `(X_lin, y_lin)` data set. Report the MSE on the full data set when trained on the full dataset. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis. print the minimal `lambda_`.\n",
    "\n",
    "How does it perform with quadratic features on this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worst-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_evaluate(regression_model, X, y):\n",
    "    \"\"\"\n",
    "    Use X and y to fit the regression_model and make prediction over the same dataset.\n",
    "    Print the error\n",
    "    \"\"\"\n",
    "    regression_model.fit(X, y)\n",
    "    yhat = regression_model.predict(X)\n",
    "    print(f'MSE: {MSE(yhat, y)}, feature type: {regression_model.ftype}')\n",
    "    \n",
    "def plot_data_and_model(regression_model, X, y):\n",
    "    \"\"\"\n",
    "    Generates a 3D plot of the regression result including the true underlying data.\n",
    "    The data points are indicated by circles, the prediction is shown as a surface\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.suptitle(f'{regression_model.ftype} Features')\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], y, marker = 'o')\n",
    "    \n",
    "    x_min = X.min(0)\n",
    "    x_max = X.max(0)\n",
    "\n",
    "    x0_grid, x1_grid = np.mgrid[x_min[0]:x_max[0]:.3, x_min[1]:x_max[1]:.3]\n",
    "\n",
    "    x_dim_0, x_dim_1 = np.shape(x0_grid)\n",
    "    x_size = np.size(x0_grid)\n",
    "\n",
    "    x0_hat = x0_grid.flatten()\n",
    "    x1_hat = x1_grid.flatten()\n",
    "    x0_hat = x0_hat.reshape((np.size(x0_hat), 1))\n",
    "    x1_hat = x1_hat.reshape((np.size(x1_hat), 1))\n",
    "    x_hat = np.append(x0_hat, x1_hat, 1)\n",
    "    x_hat_fv = make_features(x_hat, regression_model.ftype)\n",
    "    y_hat = x_hat_fv.dot(regression_model.beta)\n",
    "    y_grid = y_hat.reshape((x_dim_0, x_dim_1))\n",
    "    ax.plot_wireframe(x0_grid, x1_grid, y_grid)\n",
    "    ax.auto_scale_xyz([x_min[0], x_max[0]], [x_min[1], x_max[1]], [y.min(), y.max()])\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_zlabel('$y$')\n",
    "    \n",
    "# Implement your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-stylus",
   "metadata": {},
   "source": [
    "6) *(5 points)* Evaluate the quadratic dataset `(X_quad, y_quad)` for different values of `lambda_`. Report the MSE on the full data set when trained on the partial dataset `(X_quad_train, y_quad_train)`. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis. print the minimal `lambda_`.\n",
    "\n",
    "Plot the surface and data points of the best `lambda_` value using the function `plot_data_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "present-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-avatar",
   "metadata": {},
   "source": [
    "7) *(5 points)* Evaluate the noisy dataset `(X_noise, y_noise)` for different values of `lambda_`. Report the MSE on the full data set when trained on the partial dataset `(X_noise_train, y_noise_train)`. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis.\n",
    "\n",
    "Plot the surface and data points of the best `lambda_` value using the function `plot_data_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "functioning-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-balloon",
   "metadata": {},
   "source": [
    "## Task 3 Evaluation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-pattern",
   "metadata": {},
   "source": [
    "1) *(5 points)* What was the best choice for regularization term `lambda_` in the models above? Explain the observation from the previous task? If `lambda_` is set to zero $\\hat{\\beta}$ is not regularized, when would $\\lambda = 0$ be a good choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-bhutan",
   "metadata": {},
   "source": [
    "==> *Briefly explain the observation from the previous task.* (double klick here to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-school",
   "metadata": {},
   "source": [
    "**For all students other than B.Sc. Data Science:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-wright",
   "metadata": {},
   "source": [
    "2) *(15 points)* Implement the function `cross_validation` for `k_fold = 10` to evaluate the prediction error of your model. Report the mean squared error from cross-validation. Repeat this for different Ridge regularization parameters `lambda_` and generate a nice bar plot of the MSE for various `lambda_`. Plot the values of `lambda_` on the x-axis on a logarithmical scale and the error on the y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "refined-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(regression_model, X, y, k_fold = 10):\n",
    "    \"\"\"\n",
    "    partition data X in k_fold equal sized subsets D = {D_1, ..., D_{k_fold}}, \n",
    "        fit the model on k_fold-1 subsets (D\\D_i), \n",
    "        compute MSE on the evaluatin set (D_i),\n",
    "        return the mean MSE over all subsets D\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-return",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd047ea20cc5e615e85b5cc8db5156feca113b00d1aaa4aa4151ffe418d9ab1331a",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "47ea20cc5e615e85b5cc8db5156feca113b00d1aaa4aa4151ffe418d9ab1331a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}